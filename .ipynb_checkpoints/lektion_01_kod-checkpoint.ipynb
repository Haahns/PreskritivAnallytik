{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49246132",
   "metadata": {},
   "source": [
    "### OpenAI Gym\n",
    "\n",
    "https://www.gymlibrary.dev/\n",
    "\n",
    "### Credits\n",
    "This lecture is built on work by:\n",
    "- Leonardo A. Espinosa Ph.D.\n",
    "- Andrej Scherbakov-Parland, M.Sc.\n",
    "\n",
    "### Bibliography:\n",
    "\n",
    "For this lecture:\n",
    "\n",
    "* Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. Ch 1. MIT press, 2020.\n",
    "http://incompleteideas.net/book/RLbook2020.pdf\n",
    "\n",
    "* AlphaGo the movie https://www.youtube.com/watch?v=WXuK6gekU1Y\n",
    "\n",
    "* Yuxi Li. Deep Reinforcement Learning, 2018.\n",
    "https://arxiv.org/abs/1810.06339\n",
    "\n",
    "\n",
    "General:\n",
    "\n",
    "* Introduction to Reinforcement Learning with David Silver - https://deepmind.com/learning-resources/-introduction-reinforcement-learning-david-silver\n",
    "\n",
    "* Bostrom, Nick. Superintelligence. Dunod, 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e875a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "!pip install gym[all] seaborn pandas\n",
    "\n",
    "# VS build tools for C++ 14.00 or greater for OpenAI Gym on Windows\n",
    "#\n",
    "# Download link:\n",
    "# https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
    "#\n",
    "# Tested on Windows 10, Python 3.10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16720d51",
   "metadata": {},
   "source": [
    "### Lunar Lander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c43c2a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "env.action_space.seed(42)\n",
    "\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "for _ in range(500):\n",
    "    observation, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c2d4bc",
   "metadata": {},
   "source": [
    "# Agenten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35db1b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sketching the agent\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Agent_v0(object):\n",
    "    def __init__(self):\n",
    "        self.stateHistory = None\n",
    "\n",
    "    def chooseAction(self):\n",
    "        pass\n",
    "    \n",
    "    def updateStateHistory(self):\n",
    "        pass\n",
    "    \n",
    "    def learn(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd38b5e",
   "metadata": {},
   "source": [
    "# Milj칬n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd55ff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining our environment\n",
    "\n",
    "# 6 x 6 matrix\n",
    "# Robot enters at (0,0) - top left and Exit at (5,5) - bottom right.\n",
    "# Numpy array 0s are empty, 1s are the walls (X), 2 is robot (R).\n",
    "\n",
    "class Maze_v0(object):\n",
    "    def __init__(self):\n",
    "        self.maze = np.zeros((6,6))\n",
    "        self.maze[5,:5] = 1\n",
    "        self.maze[:4,5] = 1\n",
    "        self.maze[2,2:] = 1\n",
    "        self.maze[3,2] = 1\n",
    "        self.maze[0,0] = 2\n",
    "        self.robotPosition = (0,0)\n",
    "        \n",
    "    def isAllowedMove(self, state, action):\n",
    "        pass\n",
    "\n",
    "    def updateMaze(self, action):\n",
    "        pass\n",
    "\n",
    "    def isGameOver(self, state):\n",
    "        pass\n",
    "\n",
    "    def printMaze(self):\n",
    "        print('------------------------------------')\n",
    "        for row in self.maze:\n",
    "            for col in row:\n",
    "                if col==0:\n",
    "                    print('-',end='\\t')\n",
    "                elif col==1:\n",
    "                    print('X',end='\\t')\n",
    "                elif col==2:\n",
    "                    print('R',end='\\t')\n",
    "            print('\\n')\n",
    "        print('-------------------------------------')\n",
    "\n",
    "    def get_matrix(self):\n",
    "        return self.maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "788e35fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8079/2916543232.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Visa milj칬n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmaze\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaze_v0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmaze\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintMaze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8079/824508289.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMaze_v0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaze\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaze\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaze\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Visa milj칬n\n",
    "\n",
    "maze = Maze_v0()\n",
    "maze.printMaze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f67619",
   "metadata": {},
   "source": [
    "# Render for humans please!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc7ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a much nicer plot using seaborn -> unnecessary :)\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "a = maze.get_matrix()\n",
    "plt.show(sns.heatmap(a,cmap=\"Dark2\",cbar=False,linewidths=.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deccaea",
   "metadata": {},
   "source": [
    "# Final version of the Agent\n",
    "Fr친n v친r agent skiss tidigare g칬r vi:\n",
    "\n",
    "- actionSpace Dictionary\n",
    "    - Hur den f친r r칬ra sig (x,y) kordinater\n",
    "- chooseAction function()    \n",
    "    - M칬jligheten att v칛lja en handling\n",
    "    - randomFactor! Explore vs Exploit\n",
    "- updateStateHistory function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fac5f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final version for the agent class.\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "actionSpace = {'U': (-1,0), 'D': (1,0), 'L': (0,-1), 'R': (0,1)}     # Action space, actions as movements\n",
    "\n",
    "class Agent(object):                                                 # define my agent class    \n",
    "    def __init__(self, maze, alpha=0.15, randomFactor=0.2):          # default random vales 80% exploting / 20% exploring. alpha\n",
    "        self.stateHistory = [((0,0), 0)]                             # state, reward pairs. \n",
    "        self.G = {}                                                  # present value of expected future rewards      \n",
    "        self.randomFactor = randomFactor\n",
    "        self.alpha = alpha   \n",
    "        self.initReward(maze.allowedStates)  \n",
    "\n",
    "    def chooseAction(self, state, allowedMoves):                    # encoding physics into the environment\n",
    "        maxG = -10e15                                               # basis for comparison in the first move\n",
    "        nextMove = None \n",
    "        randomN = np.random.random()                                # pick a number from a random distribution, compares with random factor\n",
    "        if randomN < self.randomFactor:\n",
    "            nextMove = np.random.choice(allowedMoves)          \n",
    "        else:            \n",
    "            for action in allowedMoves:                              # iterate over the allowed moves and look for the\n",
    "                newState = tuple([sum(x) for x in zip(state, actionSpace[action])]) # approximation of the  reward for                                                        \n",
    "                if self.G[newState] >= maxG:                         # the new state, and then we compare it with our\n",
    "                    maxG = self.G[newState]                          # best known reward\n",
    "                    nextMove = action                                # if it is good we save it as our best reward and pick\n",
    "        return nextMove                                              # the next move as the respective action.\n",
    "\n",
    "    def initReward(self, allowedStates):\n",
    "        for state in allowedStates:     \n",
    "            self.G[state] = np.random.uniform(low=-1.0, high=-0.1)\n",
    "            \n",
    "            \n",
    "    def updateStateHistory(self, state, reward):           # To update our stateHistory, we want to pass in the new state\n",
    "        self.stateHistory.append((state, reward))          # and reward, and we simply use the append function\n",
    "                                                           # to append the state reward tuple to our list.         \n",
    "\n",
    "\n",
    "    def learn(self):    #this is called at the end of the episode when the agent has finished running the maze. \n",
    "        target = 0      # we only learn when we beat the maze\n",
    "\n",
    "        for prev, reward in reversed(self.stateHistory):                    \n",
    "            self.G[prev] = self.G[prev] + self.alpha * (target - self.G[prev])            \n",
    "            target += reward\n",
    "\n",
    "        self.stateHistory = []\n",
    "        self.randomFactor -= 10e-5                        # decrease the random factor at every step, gradually go from exploration to exploitation.\n",
    "\n",
    "        \n",
    "\n",
    "    def printG(self):\n",
    "        for i in range(6):            \n",
    "            for j in range(6):\n",
    "                if (i,j) in self.G.keys():\n",
    "                    print('%.6f' % self.G[(i,j)], end='\\t')\n",
    "                else:\n",
    "                    print('X', end='\\t\\t')\n",
    "            print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830591bc",
   "metadata": {},
   "source": [
    "# Final version of the Environment\n",
    "\n",
    "Vi modifierar v친r vakra seaborn labyrint med de sista detaljerna\n",
    "\n",
    "- Ingen tunnel effekt, vi f칬ljer fysikens lagar och ser till att agenten inte kan r칬ra sig utanf칬r \"sin\" v칛rld eller g친 in/igenom v칛ggar.\n",
    "- Vi s칛tter in till친tna handlingar (actionSpace) i milj칬n\n",
    "\n",
    "Detta 칛r ett av de enklare s칛tten att l칬sa labyrinten, dock abosolut inte det mest effektiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852f16f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final version for the environment, aka Maze\n",
    "# Define allowed moves, construct allowed states, game over function, get state, print maze and update maze.\n",
    "\n",
    "# Aditional functionalities\n",
    "# - Keep track of the number of steps\n",
    "# - Check game over.\n",
    "# - Get state / reward.\n",
    "# - Print the maze\n",
    "\n",
    "import numpy as np\n",
    "actionSpace = {'U': (-1,0), 'D': (1,0), 'L': (0,-1), 'R': (0,1)}  # action space\n",
    "\n",
    "class Maze(object):\n",
    "    def __init__(self):        \n",
    "        self.maze = np.zeros((6,6)) # 6x6 maze - exit at 5,5\n",
    "        self.maze[5, :5] = 1 \n",
    "        self.maze[:4, 5] = 1\n",
    "        self.maze[2, 2:] = 1\n",
    "        self.maze[3,2] = 1\n",
    "        self.maze[0,0] = 2\n",
    "        self.robotPosition = (0,0)\n",
    "        self.steps = 0                                          # initial steps to zero\n",
    "        self.constructAllowedStates()                           # construct the allowed states\n",
    "\n",
    "    def printMaze(self):\n",
    "        print('------------------------------------------')\n",
    "        for row in self.maze:      \n",
    "            for col in row:\n",
    "                if col == 0:                                    # empty spaces\n",
    "                    print('', end='\\t')\n",
    "                elif col == 1:                                  # walls\n",
    "                    print('X', end='\\t')\n",
    "                elif col == 2:                                  # robot\n",
    "                    print('R', end='\\t')                    \n",
    "            print('\\n')\n",
    "        print('------------------------------------------')       \n",
    "    \n",
    "    def isAllowedMove(self, state, action):\n",
    "        y, x = state\n",
    "        y += actionSpace[action][0]                             # extract the coordinates from the action space\n",
    "        x += actionSpace[action][1]\n",
    "        if y < 0 or x < 0 or y > 5 or x > 5:                    # check if the move is allowed, inside the maze\n",
    "            return False\n",
    "\n",
    "        if self.maze[y,x] == 0 or self.maze[y,x] == 2:          #check if the new state is zero (or the actual position of the robot, because not moving is valid)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def constructAllowedStates(self):                           # construct a dictionary and loop over the maze\n",
    "        allowedStates= {}\n",
    "        for y, row in enumerate(self.maze):\n",
    "            for x, col in enumerate(row):                \n",
    "                if self.maze[(y,x)] != 1:                       # It goes space by space checking if the actions are allowed, if yes it appended to allowed states dictionary\n",
    "                    allowedStates[(y,x)] = []\n",
    "                    for action in actionSpace:\n",
    "                        if self.isAllowedMove((y,x), action):\n",
    "                            allowedStates[(y,x)].append(action)\n",
    "        self.allowedStates = allowedStates\n",
    "\n",
    "    def updateMaze(self, action):                              \n",
    "        y,x = self.robotPosition\n",
    "        self.maze[y,x] = 0                                       # Get the current position of the robot and set to 0\n",
    "        y += actionSpace[action][0]                              # read the coordinates from the action space diccionary \n",
    "        x += actionSpace[action][1]               \n",
    "        self.robotPosition = (y,x)                               # updates the position of the robot\n",
    "        self.maze[y,x] = 2                                       # update the mze\n",
    "        self.steps += 1                                          # adds a new step\n",
    "\n",
    "    def isGameOver(self):                                        # Check if the position is in the exit.\n",
    "        if self.robotPosition == (5,5):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def getStateAndReward(self):                                 \n",
    "        reward = self.giveReward()\n",
    "        return self.robotPosition, reward\n",
    "\n",
    "    def giveReward(self):                                        # Gives the reward of 0 if the robots is in the exit.\n",
    "        if self.robotPosition == (5,5):\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    def get_matrix(self):\n",
    "        return self.maze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e70d169",
   "metadata": {},
   "source": [
    "## Vad har vi gjort?\n",
    "Att l칬sa labyrinten med v친r agent 칛r en god utg친ngspunkt till sv친rare problem.\n",
    "\n",
    "Vi har t칛ckt de v칛sentliga begreppen f칬r f칬rst칛rkt l칛rande (*RL*) och skapat n친got som l칛r sig fr친n de f칬rsta principerna.\n",
    "\n",
    "Allt agenter egentligen beh칬ver 칛r ett internt minne av tillst친nds- (state-) och bel칬ningsparen (rewardpairs); en uppskattning av de f칬rv칛ntade framtida bel칬ningarna (*G*) och n친gon mekanism f칬r att d친 och d친 v칛lja slumpm칛ssiga 친tg칛rder f칬r att testa dess modell av v칛rlden (*Explore vs Exploit*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac5db21",
   "metadata": {},
   "source": [
    "# Main function to use our Agent + Maze from above\n",
    "\n",
    "\n",
    "- Tv친 agenter\n",
    "- 5000 matcher\n",
    "- Prova 洧띺 = 0.1 och  洧띺 =0.99\n",
    "- Vi v칛ljer en slumpm칛ssig faktor av 0.2.\n",
    "- Vi ritar ut hur m친nga steg det tar varje robot att klara en episod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4444eb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create our main function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "maze = Maze()\n",
    "robot = Agent(maze, alpha=0.1, randomFactor=0.2)\n",
    "moveHistory = []      # keep track of the number of moves.\n",
    "for i in range(5000): # number of games\n",
    "    if i % 1000 == 0:\n",
    "        print(i)      # just to know is running.\n",
    "    while not maze.isGameOver():     # while the game is not over\n",
    "        state, _ = maze.getStateAndReward()   # gets state and reward\n",
    "        action = robot.chooseAction(state, maze.allowedStates[state]) # update the actions\n",
    "        maze.updateMaze(action)\n",
    "        state, reward = maze.getStateAndReward()\n",
    "        robot.updateStateHistory(state, reward)\n",
    "        if maze.steps > 1000:                  # cut prematurely to avoid infinite looping\n",
    "            maze.robotPosition = (5,5)\n",
    "    robot.learn()                              # learns\n",
    "    moveHistory.append(maze.steps)             # keep tracking of steps\n",
    "    maze = Maze()                              # resets the maze but not the agent.\n",
    "\n",
    "maze = Maze()\n",
    "robot = Agent(maze, alpha=0.99, randomFactor=0.2)\n",
    "moveHistory2 = []\n",
    "for i in range(5000):\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    while not maze.isGameOver():\n",
    "        state, _ = maze.getStateAndReward()\n",
    "        action = robot.chooseAction(state, maze.allowedStates[state])\n",
    "        maze.updateMaze(action)\n",
    "        state, reward = maze.getStateAndReward()\n",
    "        robot.updateStateHistory(state, reward)\n",
    "        if maze.steps > 1000:\n",
    "            maze.robotPosition = (5,5)            \n",
    "    robot.learn()\n",
    "    moveHistory2.append(maze.steps)\n",
    "    maze = Maze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375f848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A plot to show how our agent is doing!\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(211)\n",
    "plt.semilogy(moveHistory, 'b--')\n",
    "plt.legend(['alpha=0.1'])\n",
    "plt.subplot(212)\n",
    "plt.semilogy(moveHistory2, 'r--')\n",
    "plt.legend(['alpha=0.99'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6900b4e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
